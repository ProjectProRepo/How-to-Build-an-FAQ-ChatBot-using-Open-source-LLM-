{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOH07t006QbHiKQnCOfwU0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProjectProRepo/How-to-Build-an-FAQ-ChatBot-using-Open-source-LLM-/blob/main/QuizGen_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80cuDFoCTxEN"
      },
      "outputs": [],
      "source": [
        "pip install gradio transformers accelerate pypdf PyPDF2 bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "RdaGVFedT5uX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # Login with your HF token"
      ],
      "metadata": {
        "id": "6Guw0OxEUSWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n"
      ],
      "metadata": {
        "id": "IULF292gURpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract chunks from PDF\n",
        "def extract_text_chunks(pdf_file, chunk_size=1500, overlap=200):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    full_text = \"\"\n",
        "    for page in reader.pages:\n",
        "        full_text += page.extract_text() or \"\"\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(full_text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(full_text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "bD071I-8klhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find relevant chunk using simple keyword match\n",
        "def find_relevant_chunk(chunks, topic):\n",
        "    best_score = 0\n",
        "    best_chunk = \"\"\n",
        "    topic_words = set(topic.lower().split())\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_words = set(chunk.lower().split())\n",
        "        score = len(topic_words.intersection(chunk_words))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_chunk = chunk\n",
        "    return best_chunk\n"
      ],
      "metadata": {
        "id": "_61uKr1Pkm-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate quiz\n",
        "def generate_quiz_from_pdf(pdf_file, topic):\n",
        "    chunks = extract_text_chunks(pdf_file)\n",
        "    relevant_chunk = find_relevant_chunk(chunks, topic)\n",
        "\n",
        "    prompt = (\n",
        "        f\"You're an AI tutor. Based on the following content:\\n\\n\"\n",
        "        f\"{relevant_chunk}\\n\\n\"\n",
        "        f\"Create a quiz on the topic '{topic}' with 3 multiple-choice questions. \"\n",
        "        f\"Each question must have 4 options and indicate the correct answer.\"\n",
        "    )\n",
        "\n",
        "    result = llm(prompt)[0][\"generated_text\"]\n",
        "    return result.replace(prompt, \"\").strip()"
      ],
      "metadata": {
        "id": "eM6e2yXdkoUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=generate_quiz_from_pdf,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".pdf\"], label=\"Upload a PDF (up to 22MB)\"),\n",
        "        gr.Textbox(lines=1, placeholder=\"Enter topic (e.g., Recursion, Sorting, etc.)\", label=\"Quiz Topic\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"ðŸ§  PDF-to-Quiz Generator\",\n",
        "    description=\"Upload a large PDF (like a textbook or research paper) and get a quiz generated from a topic of your choice.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "id": "lGitNteZfxAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clG73OUDiBAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
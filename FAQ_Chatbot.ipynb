{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIOp2Z5dvb5o2JgMTBBhBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProjectProRepo/How-to-Build-an-FAQ-ChatBot-using-Open-source-LLM-/blob/main/FAQ_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80cuDFoCTxEN"
      },
      "outputs": [],
      "source": [
        "pip install gradio transformers accelerate pypdf PyPDF2 bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "RdaGVFedT5uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # Login with your HF token"
      ],
      "metadata": {
        "id": "6Guw0OxEUSWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n"
      ],
      "metadata": {
        "id": "IULF292gURpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and cache full PDF text (as list of chunks)\n",
        "def extract_text_chunks(pdf_file, chunk_size=1500, overlap=200):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    full_text = \"\"\n",
        "    for page in reader.pages:\n",
        "        full_text += page.extract_text() or \"\"\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(full_text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(full_text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Find best matching chunk based on query keywords\n",
        "def find_relevant_chunk(chunks, query):\n",
        "    best_score = 0\n",
        "    best_chunk = \"\"\n",
        "    query_words = set(query.lower().split())\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_words = set(chunk.lower().split())\n",
        "        score = len(query_words.intersection(chunk_words))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_chunk = chunk\n",
        "    return best_chunk\n",
        "\n",
        "# Generate answer using LLM\n",
        "def answer_query_from_pdf(pdf_file, query):\n",
        "    chunks = extract_text_chunks(pdf_file)\n",
        "    relevant_chunk = find_relevant_chunk(chunks, query)\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a helpful assistant. Based on the following document excerpt:\\n\\n\"\n",
        "        f\"{relevant_chunk}\\n\\n\"\n",
        "        f\"Answer this question: {query}\"\n",
        "    )\n",
        "\n",
        "    result = llm(prompt)[0][\"generated_text\"]\n",
        "    return result.replace(prompt, \"\").strip()\n",
        "\n",
        "# Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=answer_query_from_pdf,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".pdf\"], label=\"Upload a large PDF\"),\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask a question about the PDF...\", label=\"Your Question\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üîç Ask Questions from a Large PDF\",\n",
        "    description=\"Upload a large PDF (up to 22MB) and ask questions. The bot finds relevant text and answers using an open-source LLM.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n"
      ],
      "metadata": {
        "id": "clG73OUDiBAa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAhwO42J60z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}